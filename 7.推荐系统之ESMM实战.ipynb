{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../esmm_public手把手Guidebook/data'\n",
    "\n",
    "#User_CateIDs 等长处理参数\n",
    "User_CateIDs_map_max_len = 100\n",
    "#User_BrandIDs 等长处理参数\n",
    "User_BrandIDs_map_max_len = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载训练和测试数据\n",
    "def load_ESMM_Train_and_Test_Data():\n",
    "    #表头\n",
    "    sample_feature_columns = ['sample_id', 'click', 'buy', 'md5', 'feature_num', 'ItemID','CategoryID','ShopID','NodeID','BrandID','Com_CateID',\n",
    "                     'Com_ShopID','Com_BrandID','Com_NodeID','PID']\n",
    "    \n",
    "    common_feature_columns = ['md5', 'feature_num', 'UserID', 'User_CateIDs', 'User_ShopIDs', 'User_BrandIDs', 'User_NodeIDs', 'User_Cluster', \n",
    "                     'User_ClusterID', 'User_Gender', 'User_Age', 'User_Level1', 'User_Level2', \n",
    "                     'User_Occupation', 'User_Geo']\n",
    "    \n",
    "    #加载数据\n",
    "    # 强制转化为其中部分列为object，是因为训练和测试某些列，Pandas load类型不一致，影响后面的序列化\n",
    "    train_sample_table = pd.read_table(data_path + '/ctr_cvr_data/BuyWeight_sampled_sample_skeleton_train_sample_feature_column.csv', sep=',',\\\n",
    "                                  dtype={'ItemID': object, 'CategoryID': object, 'ShopID': object, 'PID': object},\\\n",
    "                                  header=0, names=None, engine = 'python')\n",
    "    train_common_features = pd.read_table(data_path + '/ctr_cvr_data/BuyWeight_sampled_common_features_skeleton_train_sample_feature_column.csv', sep=',', header=0, names=None, engine = 'python')\n",
    "    \n",
    "    test_sample_table = pd.read_table(data_path + '/ctr_cvr_data/BuyWeight_sampled_sample_skeleton_test_sample_feature_column.csv', sep=',', \\\n",
    "                                  dtype={'ItemID': object, 'CategoryID': object, 'ShopID': object, 'PID': object},\\\n",
    "                                  header=0, names=None, engine = 'python')\n",
    "    test_common_features = pd.read_table(data_path + '/ctr_cvr_data/BuyWeight_sampled_common_features_skeleton_test_sample_feature_column.csv', sep=',', header=0, names=None, engine = 'python')\n",
    "    \n",
    "    #ItemId 2 数据字典\n",
    "    ItemID_set = set()\n",
    "    for val in train_sample_table['ItemID'].str.split('|'):\n",
    "        ItemID_set.update(val)\n",
    "    for val in test_sample_table['ItemID'].str.split('|'):\n",
    "        ItemID_set.update(val)\n",
    "    ItemID_set.add('<PAD>')\n",
    "    ItemID2int = {val:ii for ii, val in enumerate(ItemID_set)}\n",
    "    \n",
    "    # 将训练集和测试集的数据item ID 数据转化成Map中的数据 \n",
    "    ItemID_map = {val:[ItemID2int[row] for row in val.split('|')]  \\\n",
    "                  for ii,val in enumerate(set(train_sample_table['ItemID']))}\n",
    "    test_ItemID_map = {val:[ItemID2int[row] for row in val.split('|')]  \\\n",
    "                  for ii,val in enumerate(set(test_sample_table['ItemID']))}\n",
    "    ItemID_map.update(test_ItemID_map)\n",
    "    ItemID_map_max_len = 1\n",
    "    print(\"ItemID_map max_len:\", ItemID_map_max_len)\n",
    "    for key in ItemID_map:\n",
    "        for cnt in range(ItemID_map_max_len - len(ItemID_map[key])):\n",
    "            ItemID_map[key].insert(len(ItemID_map[key]) + cnt,itemID2int['<PAD>'])\n",
    "    train_sample_table['ItemID'] = train_sample_table['ItemID'].map(ItemID_map)\n",
    "    test_sample_table['ItemID'] = test_sample_table['ItemID'].map(ItemID_map)\n",
    "    print(\"ItemID finish\")\n",
    "    \n",
    "    # User_CateIDs 2 数据字典\n",
    "    User_CateIDs_set = set()\n",
    "    for val in train_common_features['User_CateIDs'].str.split('|'):\n",
    "        User_CateIDs_set.update(val)\n",
    "    for val in test_common_features['User_CateIDs'].str.split('|'):\n",
    "        User_CateIDs_set.update(val)\n",
    "    User_CateIDs_set.add('<PAD>')\n",
    "    User_CateIDs2int = {val:ii for ii, val in enumerate(User_CateIDs_set)}\n",
    "    \n",
    "    #处理训练集和测试数据的 User_CateIDs ，转化数字&等长处理\n",
    "    User_CateIDs_map = {val:[User_CateIDs2int[row] for row in val.split('|')]  \\\n",
    "                  for ii,val in enumerate(set(train_common_features['User_CateIDs']))}\n",
    "    test_User_CateIDs_map = {val:[User_CateIDs2int[row] for row in val.split('|')]  \\\n",
    "                  for ii,val in enumerate(set(test_common_features['User_CateIDs']))}\n",
    "    User_CateIDs_map.update(test_User_CateIDs_map)\n",
    "    print(\"User_CateIDs_map max_len:\", User_CateIDs_map_max_len)\n",
    "    for key in User_CateIDs_map:\n",
    "        for cnt in range(User_CateIDs_map_max_len - len(User_CateIDs_map[key])):\n",
    "            User_CateIDs_map[key].insert(len(User_CateIDs_map[key]) + cnt,User_CateIDs2int['<PAD>'])\n",
    "    train_common_features['User_CateIDs'] = train_common_features['User_CateIDs'].map(User_CateIDs_map)\n",
    "    test_common_features['User_CateIDs'] = test_common_features['User_CateIDs'].map(User_CateIDs_map)\n",
    "    print(\"User_CateIDs finish\")\n",
    "    \n",
    "    # User_BrandIDs 2 数字字典\n",
    "    User_BrandIDs_set = set()\n",
    "    for val in train_common_features['User_BrandIDs'].str.split('|'):\n",
    "        User_BrandIDs_set.update(val)\n",
    "    for val in test_common_features['User_BrandIDs'].str.split('|'):\n",
    "        User_BrandIDs_set.update(val)\n",
    "    User_BrandIDs_set.add('<PAD>')\n",
    "    User_BrandIDs2int = {val:ii for ii, val in enumerate(User_BrandIDs_set)}\n",
    "    \n",
    "    # 处理训练数据和测试数据的 User_BrandIDs  转化数字&等长处理\n",
    "    User_BrandIDs_map = {val:[User_BrandIDs2int[row] for row in val.split('|')]  \\\n",
    "                  for ii,val in enumerate(set(train_common_features['User_BrandIDs']))}\n",
    "    test_User_BrandIDs_map = {val:[User_BrandIDs2int[row] for row in val.split('|')]  \\\n",
    "                  for ii,val in enumerate(set(test_common_features['User_BrandIDs']))}\n",
    "    User_BrandIDs_map.update(test_User_BrandIDs_map)\n",
    "    print(\"User_BrandIDs_map max_len:\", User_BrandIDs_map_max_len)\n",
    "    for key in User_BrandIDs_map:\n",
    "        for cnt in range(User_BrandIDs_map_max_len - len(User_BrandIDs_map[key])):\n",
    "            User_BrandIDs_map[key].insert(len(User_BrandIDs_map[key]) + cnt,User_BrandIDs2int['<PAD>'])\n",
    "    train_common_features['User_BrandIDs'] = train_common_features['User_BrandIDs'].map(User_BrandIDs_map)\n",
    "    test_common_features['User_BrandIDs'] = test_common_features['User_BrandIDs'].map(User_BrandIDs_map)\n",
    "    print(\"User_BrandIDs finish\")\n",
    "    \n",
    "    #UserId 2 数字字典\n",
    "    UserID_set = set()\n",
    "    for val in train_common_features['UserID']:\n",
    "        UserID_set.add(val)\n",
    "    for val in test_common_features['UserID']:\n",
    "        UserID_set.add(val)\n",
    "    UserID2int = {val:ii for ii, val in enumerate(UserID_set)}\n",
    "    \n",
    "    #处理训练数据和测试数据的 userID 转化数字\n",
    "    UserID_map_max_len = 1\n",
    "    print(\"UserID_map max_len:\", UserID_map_max_len)\n",
    "    train_common_features['UserID'] = train_common_features['UserID'].map(UserID2int)\n",
    "    test_common_features['UserID'] = test_common_features['UserID'].map(UserID2int)\n",
    "    print(\"UserID finish\")\n",
    "    \n",
    "    # User_Cluster 2 数据字典\n",
    "    User_Cluster_set = set()\n",
    "    for val in train_common_features['User_Cluster']:\n",
    "        User_Cluster_set.add(val)\n",
    "    for val in test_common_features['User_Cluster']:\n",
    "        User_Cluster_set.add(val)\n",
    "    User_Cluster2int = {val:ii for ii, val in enumerate(User_Cluster_set)}\n",
    "    \n",
    "    # 处理训练集和测试集上的User_Cluster 转化数字\n",
    "    User_Cluster_map_max_len = 1\n",
    "    print(\"User_Cluster_map max_len:\", User_Cluster_map_max_len)\n",
    "    train_common_features['User_Cluster'] = train_common_features['User_Cluster'].map(User_Cluster2int)\n",
    "    test_common_features['User_Cluster'] = test_common_features['User_Cluster'].map(User_Cluster2int)\n",
    "    print(\"User_Cluster finish\")\n",
    "    \n",
    "    # CategoryID 2 数据字典\n",
    "    CategoryID_set = set()\n",
    "    for val in train_sample_table['CategoryID']:\n",
    "        CategoryID_set.add(val)\n",
    "    for val in test_sample_table['CategoryID']:\n",
    "        CategoryID_set.add(val)\n",
    "    CategoryID2int = {val:ii for ii, val in enumerate(CategoryID_set)}\n",
    "    \n",
    "    #处理数据中的 CategoryID 转化数字\n",
    "    CategoryID_map_max_len = 1\n",
    "    print(\"CategoryID_map max_len:\", CategoryID_map_max_len)\n",
    "    train_sample_table['CategoryID'] = train_sample_table['CategoryID'].map(CategoryID2int)\n",
    "    test_sample_table['CategoryID'] = test_sample_table['CategoryID'].map(CategoryID2int)\n",
    "    print(\"CategoryID finish\")\n",
    "    \n",
    "    # ShopID 2 数据字典 \n",
    "    ShopID_set = set()\n",
    "    for val in train_sample_table['ShopID']:\n",
    "        ShopID_set.add(val)\n",
    "    for val in test_sample_table['ShopID']:\n",
    "        ShopID_set.add(val)\n",
    "    ShopID2int = {val:ii for ii, val in enumerate(ShopID_set)}\n",
    "    \n",
    "    #处理训练集和测试集中的  ShopID 转化数字\n",
    "    ShopID_map_max_len = 1\n",
    "    print(\"ShopID_map max_len:\", ShopID_map_max_len)\n",
    "    train_sample_table['ShopID'] = train_sample_table['ShopID'].map(ShopID2int)\n",
    "    test_sample_table['ShopID'] = test_sample_table['ShopID'].map(ShopID2int)\n",
    "    print(\"ShopID finish\")\n",
    "    \n",
    "    \n",
    "    #BrandID 2 数字字典\n",
    "    BrandID_set = set()\n",
    "    for val in train_sample_table['BrandID']:\n",
    "        BrandID_set.add(val)\n",
    "    for val in test_sample_table['BrandID']:\n",
    "        BrandID_set.add(val)\n",
    "    BrandID2int = {val:ii for ii, val in enumerate(BrandID_set)}\n",
    "    \n",
    "    #处理训练集和测试集中的  BrandID 转化数字\n",
    "    BrandID_map_max_len = 1\n",
    "    print(\"BrandID_map max_len:\", UserID_map_max_len)\n",
    "    train_sample_table['BrandID'] = train_sample_table['BrandID'].map(BrandID2int)\n",
    "    test_sample_table['BrandID'] = test_sample_table['BrandID'].map(BrandID2int)\n",
    "    print(\"BrandID finish\")\n",
    "\n",
    "\n",
    "    # Com_CateID 2 数字字典\n",
    "    Com_CateID_set = set()\n",
    "    for val in train_sample_table['Com_CateID']:\n",
    "        Com_CateID_set.add(val)\n",
    "    for val in test_sample_table['Com_CateID']:\n",
    "        Com_CateID_set.add(val)\n",
    "    Com_CateID2int = {val:ii for ii, val in enumerate(Com_CateID_set)}\n",
    "    \n",
    "    #处理训练集和测试集中的  Com_CateID 转化数字\n",
    "    Com_CateID_map_max_len = 1\n",
    "    print(\"Com_CateID_map max_len:\", Com_CateID_map_max_len)\n",
    "    train_sample_table['Com_CateID'] = train_sample_table['Com_CateID'].map(Com_CateID2int)\n",
    "    test_sample_table['Com_CateID'] = test_sample_table['Com_CateID'].map(Com_CateID2int)\n",
    "    print(\"Com_CateID finish\")\n",
    "    \n",
    "    #Com_ShopID 2 数字字典\n",
    "    Com_ShopID_set = set()\n",
    "    for val in train_sample_table['Com_ShopID']:\n",
    "        Com_ShopID_set.add(val)\n",
    "    for val in test_sample_table['Com_ShopID']:\n",
    "        Com_ShopID_set.add(val)\n",
    "    Com_ShopID2int = {val:ii for ii, val in enumerate(Com_ShopID_set)}\n",
    "    \n",
    "    #处理训练集和测试集中的  Com_ShopID 转化数字\n",
    "    Com_ShopID_map_max_len = 1\n",
    "    print(\"Com_ShopID_map max_len:\", Com_ShopID_map_max_len)\n",
    "    train_sample_table['Com_ShopID'] = train_sample_table['Com_ShopID'].map(Com_ShopID2int)\n",
    "    test_sample_table['Com_ShopID'] = test_sample_table['Com_ShopID'].map(Com_ShopID2int)\n",
    "    print(\"Com_ShopID finish\")\n",
    "    \n",
    "    #Com_BrandID 2 数字字典\n",
    "    Com_BrandID_set = set()\n",
    "    for val in train_sample_table['Com_BrandID']:\n",
    "        Com_BrandID_set.add(val)\n",
    "    for val in test_sample_table['Com_BrandID']:\n",
    "        Com_BrandID_set.add(val)\n",
    "    Com_BrandID2int = {val:ii for ii, val in enumerate(Com_BrandID_set)}\n",
    "    \n",
    "    #处理训练集和测试集中的  Com_BrandID 转化数字\n",
    "    Com_BrandID_map_max_len = 1\n",
    "    print(\"Com_BrandID_map max_len:\", UserID_map_max_len)\n",
    "    train_sample_table['Com_BrandID'] = train_sample_table['Com_BrandID'].map(Com_BrandID2int)\n",
    "    test_sample_table['Com_BrandID'] = test_sample_table['Com_BrandID'].map(Com_BrandID2int)\n",
    "    print(\"Com_BrandID finish\")\n",
    "    \n",
    "    #PID 2 数字字典\n",
    "    PID_set = set()\n",
    "    for val in train_sample_table['PID']:\n",
    "        PID_set.add(val)\n",
    "    for val in test_sample_table['PID']:\n",
    "        PID_set.add(val)\n",
    "    PID2int = {val:ii for ii, val in enumerate(PID_set)}\n",
    "    \n",
    "    #处理训练集和测试集中的  PID 转化数字\n",
    "    PID_map_max_len = 1\n",
    "    print(\"PID_map max_len:\", PID_map_max_len)\n",
    "    train_sample_table['PID'] = train_sample_table['PID'].map(PID2int)\n",
    "    test_sample_table['PID'] = test_sample_table['PID'].map(PID2int)\n",
    "    print(\"PID finish\")\n",
    "    \n",
    "    #按照md5合并两个表\n",
    "    train_data = pd.merge(train_sample_table, train_common_features, on='md5',how='inner')\n",
    "    test_data = pd.merge(test_sample_table, test_common_features, on='md5',how='inner')\n",
    "    print(\"Sample/Common Merged\")\n",
    "    \n",
    "    #将数据分成 x 和 y\n",
    "    feature_fields = ['UserID','ItemID','User_Cluster', 'CategoryID','ShopID',\\\n",
    "                      'BrandID','Com_CateID','Com_ShopID','Com_BrandID','PID','User_CateIDs','User_BrandIDs']\n",
    "    target_fields = ['click','buy']\n",
    "    train_features_pd, train_targets_pd = train_data[feature_fields], train_data[target_fields]\n",
    "    train_features = train_features_pd.values\n",
    "    train_targets_values = train_targets_pd.values\n",
    "    \n",
    "    test_features_pd, test_targets_pd = test_data[feature_fields], test_data[target_fields]\n",
    "    test_features = test_features_pd.values\n",
    "    test_targets_values = test_targets_pd.values\n",
    "    \n",
    "    # 返回数据\n",
    "    return UserID_map_max_len, ItemID_map_max_len, User_Cluster_map_max_len, \\\n",
    "User_CateIDs_map_max_len, User_BrandIDs_map_max_len, \\\n",
    "CategoryID_map_max_len, ShopID_map_max_len, BrandID_map_max_len, Com_CateID_map_max_len,\\\n",
    "Com_ShopID_map_max_len, Com_BrandID_map_max_len, PID_map_max_len, UserID2int, ItemID2int,\\\n",
    "User_Cluster2int, User_CateIDs2int, User_BrandIDs2int,  CategoryID2int, ShopID2int, BrandID2int, Com_CateID2int, \\\n",
    "Com_ShopID2int, Com_BrandID2int, PID2int, train_features, train_targets_values, train_data, \\\n",
    "test_features, test_targets_values, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('数据处理开始！')\n",
    "UserID_map_max_len, ItemID_map_max_len, User_Cluster_map_max_len, User_CateIDs_map_max_len, User_BrandIDs_map_max_len, CategoryID_map_max_len, ShopID_map_max_len, BrandID_map_max_len, Com_CateID_map_max_len,Com_ShopID_map_max_len, Com_BrandID_map_max_len, PID_map_max_len, UserID2int, ItemID2int,User_Cluster2int, User_CateIDs2int, User_BrandIDs2int,  CategoryID2int, ShopID2int, BrandID2int, Com_CateID2int, Com_ShopID2int, Com_BrandID2int, PID2int, train_features, train_targets_values, train_data, test_features, test_targets_values, test_data = load_ESMM_Train_and_Test_Data()\n",
    "print('加载数据完成！')\n",
    "pickle.dump((UserID_map_max_len, ItemID_map_max_len, User_Cluster_map_max_len, User_CateIDs_map_max_len, User_BrandIDs_map_max_len, CategoryID_map_max_len, ShopID_map_max_len, BrandID_map_max_len, Com_CateID_map_max_len,Com_ShopID_map_max_len, Com_BrandID_map_max_len, PID_map_max_len, UserID2int, ItemID2int,User_Cluster2int, User_CateIDs2int, User_BrandIDs2int,  CategoryID2int, ShopID2int, BrandID2int, Com_CateID2int, Com_ShopID2int, Com_BrandID2int, PID2int, train_features, train_targets_values, train_data, test_features, test_targets_values, test_data), open('./ESMM_data/preprocess.p', 'wb'))\n",
    "print('保存数据完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从本地读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取完成！\n"
     ]
    }
   ],
   "source": [
    "UserID_map_max_len, ItemID_map_max_len, User_Cluster_map_max_len, \\\n",
    "User_CateIDs_map_max_len, User_BrandIDs_map_max_len, \\\n",
    "CategoryID_map_max_len, ShopID_map_max_len, BrandID_map_max_len, Com_CateID_map_max_len,\\\n",
    "Com_ShopID_map_max_len, Com_BrandID_map_max_len, PID_map_max_len, UserID2int, ItemID2int,\\\n",
    "User_Cluster2int, User_CateIDs2int, User_BrandIDs2int,  CategoryID2int, ShopID2int, BrandID2int, Com_CateID2int, \\\n",
    "Com_ShopID2int, Com_BrandID2int, PID2int, train_features, train_targets_values, train_data, \\\n",
    "test_features, test_targets_values, test_data = pickle.load(open('./ESMM_data/preprocess.p', mode='rb'))\n",
    "print('读取完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/esmm.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整个Batch 的AUC计算 ， 适合CTR  ，CTCVR \n",
    "def calc_auc(raw_arr):\n",
    "    \n",
    "    #降序排序\n",
    "    arr = sorted(raw_arr,key= lambda d:d[2])\n",
    "    \n",
    "    auc = 0.0\n",
    "    fp1, tp1, fp2, tp2 = 0.0, 0.0, 0.0, 0.0\n",
    "    for record in arr:\n",
    "        fp2 += record[0] # noclick\n",
    "        tp2 += record[1] # click\n",
    "        auc += (fp2 - fp1) * (tp2 + tp1)\n",
    "        fp1, tp1 = fp2, tp2\n",
    "        \n",
    "    # if all nonclick or click, disgard\n",
    "    threshold = len(arr) - 1e-3\n",
    "    if tp2 > threshold or fp2 > threshold:\n",
    "        return -0.5\n",
    "    \n",
    "    if tp2 * fp2 > 0.0:  # normal auc\n",
    "        return (1.0 - auc / (2.0 * tp2 * fp2))\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "### AUC 带Filter计算（CVR AUC只需要计算Click=1的样本子集）\n",
    "def calc_auc_with_filter(raw_arr, filter_arr):\n",
    "    ## get filter array row indexes\n",
    "    filter_index = np.nonzero(filter_arr)[0].tolist()\n",
    "    input_arr = [raw_arr[index] for index in filter_index]\n",
    "    auc_val = calc_auc(input_arr)\n",
    "    return auc_val  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造网络使用的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 12\n",
    "#userID个数\n",
    "UserID_max = max(UserID2int.values()) + 1 \n",
    "#itemID个数\n",
    "ItemID_max = max(ItemID2int.values()) + 1 \n",
    "User_Cluster_max = max(User_Cluster2int.values()) + 1 \n",
    "User_CateIDs_max = max(User_CateIDs2int.values()) + 1 \n",
    "User_BrandIDs_max = max(User_BrandIDs2int.values()) + 1 \n",
    "\n",
    "CategoryID_max = max(CategoryID2int.values()) + 1 \n",
    "ShopID_max = max(ShopID2int.values()) + 1 \n",
    "BrandID_max = max(BrandID2int.values()) + 1 \n",
    "Com_CateID_max = max(Com_CateID2int.values()) + 1 \n",
    "Com_ShopID_max = max(Com_ShopID2int.values()) + 1 \n",
    "Com_BrandID_max = max(Com_BrandID2int.values()) + 1 \n",
    "PID_max = max(PID2int.values()) + 1 \n",
    "\n",
    "#变长特征pooling方式\n",
    "combiner = \"sum\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    UserID = tf.placeholder(tf.int32, [None, 1], name=\"UserID\")\n",
    "    ItemID = tf.placeholder(tf.int32, [None, 1], name=\"ItemID\")\n",
    "    User_Cluster = tf.placeholder(tf.int32, [None, 1], name=\"User_Cluster\")\n",
    "    User_CateIDs = tf.placeholder(tf.int32, [None, 100], name=\"User_CateIDs\")\n",
    "    User_BrandIDs = tf.placeholder(tf.int32, [None, 100], name=\"User_BrandIDs\")\n",
    "    \n",
    "    CategoryID = tf.placeholder(tf.int32, [None, 1], name=\"CategoryID\")\n",
    "    ShopID = tf.placeholder(tf.int32, [None, 1], name=\"ShopID\")\n",
    "    BrandID = tf.placeholder(tf.int32, [None, 1], name=\"BrandID\")\n",
    "    Com_CateID = tf.placeholder(tf.int32, [None, 1], name=\"Com_CateID\")\n",
    "    Com_ShopID = tf.placeholder(tf.int32, [None, 1], name=\"Com_ShopID\")\n",
    "    Com_BrandID = tf.placeholder(tf.int32, [None, 1], name=\"Com_BrandID\")\n",
    "    PID = tf.placeholder(tf.int32, [None, 1], name=\"PID\")\n",
    "    \n",
    "    targets = tf.placeholder(tf.float32, [None, 2], name=\"targets\")#是否点击，是否购买\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    return  UserID, ItemID, User_Cluster, CategoryID, ShopID, BrandID, Com_CateID,\\\n",
    "            Com_ShopID, Com_BrandID, PID, User_CateIDs, User_BrandIDs, targets, LearningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_embedding_layers(UserID, ItemID, User_Cluster, CategoryID, ShopID, BrandID, Com_CateID,\\\n",
    "            Com_ShopID, Com_BrandID, PID, User_CateIDs, User_BrandIDs):\n",
    "\n",
    "    UserID_embed_matrix = tf.Variable(tf.random_normal([UserID_max, embed_dim], 0, 0.001))\n",
    "    UserID_embed_layer = tf.nn.embedding_lookup(UserID_embed_matrix, UserID)\n",
    "    if combiner == \"sum\":\n",
    "        UserID_embed_layer = tf.reduce_sum(UserID_embed_layer, axis=1, keep_dims=True)\n",
    "        \n",
    "    ItemID_embed_matrix = tf.Variable(tf.random_uniform([ItemID_max, embed_dim], 0, 0.001))\n",
    "    ItemID_embed_layer = tf.nn.embedding_lookup(ItemID_embed_matrix, ItemID)\n",
    "    if combiner == \"sum\":\n",
    "        ItemID_embed_layer = tf.reduce_sum(ItemID_embed_layer, axis=1, keep_dims=True)\n",
    "\n",
    "    User_Cluster_embed_matrix = tf.Variable(tf.random_uniform([User_Cluster_max, embed_dim], 0, 0.001))\n",
    "    User_Cluster_embed_layer = tf.nn.embedding_lookup(User_Cluster_embed_matrix, User_Cluster)\n",
    "    if combiner == \"sum\":\n",
    "        User_Cluster_embed_layer = tf.reduce_sum(User_Cluster_embed_layer, axis=1, keep_dims=True)\n",
    "        \n",
    "    User_CateIDs_embed_matrix = tf.Variable(tf.random_uniform([User_CateIDs_max, embed_dim], 0, 0.001))\n",
    "    User_CateIDs_embed_layer = tf.nn.embedding_lookup(User_CateIDs_embed_matrix, User_CateIDs)\n",
    "    if combiner == \"sum\":\n",
    "        User_CateIDs_embed_layer = tf.reduce_sum(User_CateIDs_embed_layer, axis=1, keep_dims=True)\n",
    "        \n",
    "    User_BrandIDs_embed_matrix = tf.Variable(tf.random_uniform([User_BrandIDs_max, embed_dim], 0, 0.001))\n",
    "    User_BrandIDs_embed_layer = tf.nn.embedding_lookup(User_BrandIDs_embed_matrix, User_BrandIDs)\n",
    "    if combiner == \"sum\":\n",
    "        User_BrandIDs_embed_layer = tf.reduce_sum(User_BrandIDs_embed_layer, axis=1, keep_dims=True)\n",
    "        \n",
    "    CategoryID_embed_matrix = tf.Variable(tf.random_uniform([CategoryID_max, embed_dim], 0, 0.001))\n",
    "    CategoryID_embed_layer = tf.nn.embedding_lookup(CategoryID_embed_matrix, CategoryID)\n",
    "    if combiner == \"sum\":\n",
    "        CategoryID_embed_layer = tf.reduce_sum(CategoryID_embed_layer, axis=1, keep_dims=True)\n",
    "    \n",
    "    ShopID_embed_matrix = tf.Variable(tf.random_uniform([ShopID_max, embed_dim], 0, 0.001))\n",
    "    ShopID_embed_layer = tf.nn.embedding_lookup(ShopID_embed_matrix, ShopID)\n",
    "    if combiner == \"sum\":\n",
    "        ShopID_embed_layer = tf.reduce_sum(ShopID_embed_layer, axis=1, keep_dims=True)\n",
    "\n",
    "    BrandID_embed_matrix = tf.Variable(tf.random_uniform([BrandID_max, embed_dim], 0, 0.001))\n",
    "    BrandID_embed_layer = tf.nn.embedding_lookup(BrandID_embed_matrix, BrandID)\n",
    "    if combiner == \"sum\":\n",
    "        BrandID_embed_layer = tf.reduce_sum(BrandID_embed_layer, axis=1, keep_dims=True)\n",
    "\n",
    "    Com_CateID_embed_matrix = tf.Variable(tf.random_uniform([Com_CateID_max, embed_dim], 0, 0.001))\n",
    "    Com_CateID_embed_layer = tf.nn.embedding_lookup(Com_CateID_embed_matrix, Com_CateID)\n",
    "    if combiner == \"sum\":\n",
    "        Com_CateID_embed_layer = tf.reduce_sum(Com_CateID_embed_layer, axis=1, keep_dims=True)\n",
    "\n",
    "    Com_ShopID_embed_matrix = tf.Variable(tf.random_uniform([Com_ShopID_max, embed_dim], 0, 0.001))\n",
    "    Com_ShopID_embed_layer = tf.nn.embedding_lookup(Com_ShopID_embed_matrix, Com_ShopID)\n",
    "    if combiner == \"sum\":\n",
    "        Com_ShopID_embed_layer = tf.reduce_sum(Com_ShopID_embed_layer, axis=1, keep_dims=True)\n",
    "\n",
    "    Com_BrandID_embed_matrix = tf.Variable(tf.random_uniform([Com_BrandID_max, embed_dim], 0, 0.001))\n",
    "    Com_BrandID_embed_layer = tf.nn.embedding_lookup(Com_BrandID_embed_matrix, Com_BrandID)\n",
    "    if combiner == \"sum\":\n",
    "        Com_BrandID_embed_layer = tf.reduce_sum(Com_BrandID_embed_layer, axis=1, keep_dims=True)\n",
    "\n",
    "\n",
    "    PID_embed_matrix = tf.Variable(tf.random_uniform([PID_max, embed_dim], 0, 0.001))\n",
    "    PID_embed_layer = tf.nn.embedding_lookup(PID_embed_matrix, PID)\n",
    "    if combiner == \"sum\":\n",
    "        PID_embed_layer = tf.reduce_sum(PID_embed_layer, axis=1, keep_dims=True)\n",
    "\n",
    "    # 数据量较小，选择User Cluster and 其他一些较低低维度特征  数据拼接\n",
    "    esmm_embedding_layer = tf.concat([User_CateIDs_embed_layer,\\\n",
    "                                      User_BrandIDs_embed_layer,\\\n",
    "                                      ItemID_embed_layer,\\\n",
    "                                     CategoryID_embed_layer,\\\n",
    "                                     Com_CateID_embed_layer,\\\n",
    "                                      PID_embed_layer,], 2)\n",
    "    esmm_embedding_layer = tf.reshape(esmm_embedding_layer, [-1, embed_dim * 6]) ## ？\n",
    "    return esmm_embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络其他层参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTR模块 接三层全链接层\n",
    "def define_ctr_layer(esmm_embedding_layer):\n",
    "    ctr_layer_1 = tf.layers.dense(esmm_embedding_layer, 200, activation=tf.nn.relu)\n",
    "    ctr_layer_2 = tf.layers.dense(ctr_layer_1, 80, activation=tf.nn.relu)\n",
    "    ctr_layer_3 = tf.layers.dense(ctr_layer_2, 2) # [nonclick, click]\n",
    "    ctr_prob = tf.nn.softmax(ctr_layer_3) + 0.00000001\n",
    "    return ctr_prob\n",
    "\n",
    "# CVR模块 接三层券链接\n",
    "def define_cvr_layer(esmm_embedding_layer):\n",
    "    cvr_layer_1 = tf.layers.dense(esmm_embedding_layer, 200, activation=tf.nn.relu)\n",
    "    cvr_layer_2 = tf.layers.dense(cvr_layer_1, 80, activation=tf.nn.relu)\n",
    "    cvr_layer_3 = tf.layers.dense(cvr_layer_2, 2) # [nonbuy, buy]\n",
    "    cvr_prob = tf.nn.softmax(cvr_layer_3) + 0.00000001\n",
    "    return cvr_prob\n",
    "\n",
    "\n",
    "# 定义前两层 ctr,cvr 共享\n",
    "def define_ctr_cvr_layer(esmm_embedding_layer):\n",
    "    layer_1 = tf.layers.dense(esmm_embedding_layer, 128 , activation=tf.nn.relu)\n",
    "    layer_2 = tf.layers.dense(layer_1, 16, activation=tf.nn.relu)\n",
    "    layer_ctr = tf.layers.dense(layer_2, 2)\n",
    "    ctr_prob = tf.nn.softmax(layer_ctr) + 0.00000001\n",
    "    layer_cvr = tf.layers.dense(layer_2, 2)\n",
    "    cvr_prob = tf.nn.softmax(layer_cvr) + 0.00000001\n",
    "    return ctr_prob, cvr_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    tf.reset_default_graph()\n",
    "    train_graph = tf.Graph()\n",
    "    \n",
    "    #定义占位符\n",
    "    UserID, ItemID, User_Cluster, CategoryID, ShopID, BrandID, Com_CateID,Com_ShopID, Com_BrandID, PID, User_CateIDs, User_BrandIDs, targets, lr = get_inputs()\n",
    "    \n",
    "    #定义embedding 层\n",
    "    esmm_embedding_layer = define_embedding_layers(UserID, ItemID, User_Cluster, CategoryID, ShopID, BrandID, Com_CateID,\\\n",
    "            Com_ShopID, Com_BrandID, PID, User_CateIDs, User_BrandIDs)\n",
    "    \n",
    "    # 定义CTR层CVR层\n",
    "    ctr_prob, cvr_prob = define_ctr_cvr_layer(esmm_embedding_layer)\n",
    "    \n",
    "    # 定义损失函数\n",
    "    with tf.name_scope('loss'):\n",
    "        # 训练预测的ctr,cvr\n",
    "        ctr_prob_one = tf.slice(ctr_prob,[0,1],[-1,1]) # shape [batch_size,1]\n",
    "        cvr_prob_one = tf.slice(cvr_prob,[0,1],[-1,1]) # shape [batch_size,1]\n",
    "        \n",
    "        ctcvr_prob_one = ctr_prob_one * cvr_prob_one # [ctr*cvr]\n",
    "        ctcvr_prob = tf.concat([1 - ctcvr_prob_one, ctcvr_prob_one], axis=1)\n",
    "        \n",
    "        #训练数据的ctr , cvr\n",
    "        ctr_label =  tf.slice(targets, [0,0], [-1, 1]) # target: [click, buy]\n",
    "        ctr_label = tf.concat([1 - ctr_label, ctr_label], axis=1) # [1-click, click]\n",
    "        \n",
    "        cvr_label = tf.slice(targets, [0,1], [-1, 1])\n",
    "        ctcvr_label = tf.concat([1 - cvr_label, cvr_label], axis=1)\n",
    "        \n",
    "        # 单列，判断Click是否=1\n",
    "        ctr_clk = tf.slice(targets, [0,0], [-1, 1])\n",
    "        ctr_clk_dup = tf.concat([ctr_clk, ctr_clk], axis=1)\n",
    "        \n",
    "        #交叉熵损失函数：-y*log(p)-(1-y)*log(1-p)\n",
    "        # clicked subset CVR loss\n",
    "        cvr_loss = - tf.multiply(tf.log(cvr_prob) * ctcvr_label, ctr_clk_dup)\n",
    "        # batch CTR loss\n",
    "        ctr_loss = - tf.log(ctr_prob) * ctr_label # \n",
    "        # batch CTCVR loss\n",
    "        ctcvr_loss = - tf.log(ctcvr_prob) * ctcvr_label\n",
    "        \n",
    "        loss = tf.reduce_mean(ctr_loss + ctcvr_loss + cvr_loss)\n",
    "        \n",
    "        ctr_loss = tf.reduce_mean(ctr_loss)\n",
    "        cvr_loss = tf.reduce_mean(cvr_loss)\n",
    "        ctcvr_loss = tf.reduce_mean(ctcvr_loss)\n",
    "        \n",
    "    #优化损失\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "    \n",
    "    return train_graph,\\\n",
    "             UserID, ItemID, User_Cluster, CategoryID,\\\n",
    "             ShopID, BrandID, Com_CateID,Com_ShopID, \\\n",
    "             Com_BrandID, PID, User_CateIDs, User_BrandIDs, targets, lr,\\\n",
    "             global_step, loss, ctr_loss, cvr_loss, ctcvr_loss, \\\n",
    "             ctr_prob, cvr_prob, ctcvr_prob,\\\n",
    "             ctr_label, ctcvr_label, ctcvr_label, ctr_clk, \\\n",
    "             train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 1\n",
    "# Batch Size\n",
    "batch_size = 10000\n",
    "\n",
    "# Test Batch Size\n",
    "test_batch_size = 10000\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "show_test_every_n_batches = 10\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练网路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"UserID:0\", shape=(?, 1), dtype=int32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/anaconda2/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    929\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m--> 930\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m    931\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2413\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2414\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2492\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"UserID:0\", shape=(?, 1), dtype=int32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0a07400c503c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 lr: learning_rate}\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ctr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cvr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ctcvr_loss\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mtrain_ctr_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cvr_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ctcvr_prob\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mtrain_ctr_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cvr_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ctcvr_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ctr_click\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctcvr_loss\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mctr_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvr_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctcvr_prob\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0mctr_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctcvr_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctcvr_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctr_clk\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    931\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m--> 933\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"UserID:0\", shape=(?, 1), dtype=int32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "# 存储训练数据\n",
    "losses = {'train':[], 'test':[]}\n",
    "ctr_auc_stat = {'train':[], 'test':[]}\n",
    "cvr_auc_stat = {'train':[], 'test':[]}\n",
    "ctcvr_auc_stat = {'train':[], 'test':[]}\n",
    "\n",
    "#获取训练网络需要的节点\n",
    "train_graph,\\\n",
    "UserID, ItemID, User_Cluster, CategoryID,\\\n",
    "ShopID, BrandID, Com_CateID,Com_ShopID, \\\n",
    "Com_BrandID, PID, User_CateIDs, User_BrandIDs, targets, lr,\\\n",
    "global_step, loss, ctr_loss, cvr_loss, ctcvr_loss, \\\n",
    "ctr_prob, cvr_prob, ctcvr_prob,\\\n",
    "ctr_label, ctcvr_label, ctcvr_label, ctr_clk, \\\n",
    "train_op = build_graph()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    '''\n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    \n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "    \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "    '''\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver = tf.train.Saver()\n",
    "    \n",
    "    # 训练集和测试集用两天的数据，前一天训练，后一天测试\n",
    "    train_X, train_y = train_features, train_targets_values \n",
    "    test_X, test_y = test_features, test_targets_values \n",
    "    \n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        train_ctr_auc_arr = []\n",
    "        train_cvr_auc_arr = []\n",
    "        train_ctcvr_auc_arr = []\n",
    "        \n",
    "        test_ctr_auc_arr = []\n",
    "        test_cvr_auc_arr = []\n",
    "        test_ctcvr_auc_arr = []\n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, test_batch_size)\n",
    "        \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "            \n",
    "            item_id = np.zeros([batch_size, 1])\n",
    "            for i in range(batch_size):\n",
    "                item_id[i] = x.take(1,1)[i]\n",
    "                \n",
    "            #User_CateIDs, User_BrandIDs\n",
    "            user_cateids = np.zeros([batch_size, User_CateIDs_map_max_len])\n",
    "            for i in range(batch_size):\n",
    "                user_cateids[i] = x.take(10,1)[i]\n",
    "            user_brandids = np.zeros([batch_size, User_BrandIDs_map_max_len])\n",
    "            for i in range(batch_size):\n",
    "                user_brandids[i] = x.take(11,1)[i]\n",
    "                \n",
    "        feed = {\n",
    "                UserID : np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                ItemID: item_id,\n",
    "                User_Cluster : np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                CategoryID : np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                ShopID : np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                BrandID : np.reshape(x.take(5,1), [batch_size, 1]),\n",
    "                Com_CateID : np.reshape(x.take(6,1), [batch_size, 1]),\n",
    "                Com_ShopID : np.reshape(x.take(7,1), [batch_size, 1]),\n",
    "                Com_BrandID : np.reshape(x.take(8,1), [batch_size, 1]),\n",
    "                PID : np.reshape(x.take(9,1), [batch_size, 1]),\n",
    "                User_CateIDs: user_cateids,\n",
    "                User_BrandIDs: user_brandids,\n",
    "                targets: y,\n",
    "                lr: learning_rate}\n",
    "        step, train_loss, train_ctr_loss, train_cvr_loss, train_ctcvr_loss, \\\n",
    "                train_ctr_prob, train_cvr_prob, train_ctcvr_prob, \\\n",
    "                train_ctr_label, train_cvr_label, train_ctcvr_label, train_ctr_click,\\\n",
    "                _ = sess.run([global_step, loss, ctr_loss, cvr_loss, ctcvr_loss, \\\n",
    "                                    ctr_prob, cvr_prob, ctcvr_prob,\\\n",
    "                                    ctr_label, ctcvr_label, ctcvr_label, ctr_clk, \\\n",
    "                                    train_op], feed) \n",
    "        losses['train'].append(train_loss)   \n",
    "            \n",
    "        print(\"train batch click num:\", len(np.nonzero(y[:,0:1])[0]), \n",
    "                    \" buy num:\", len(np.nonzero(y[:,1:2])[0]))\n",
    "            \n",
    "        ctr_input_arr = np.concatenate((train_ctr_label, train_ctr_prob[:, 1:2]), axis=1)\n",
    "        train_ctr_auc = calc_auc(ctr_input_arr)\n",
    "        if train_ctr_auc > 0:\n",
    "            train_ctr_auc_arr.append(train_ctr_auc)\n",
    "\n",
    "        cvr_input_arr = np.concatenate((train_cvr_label, train_cvr_prob[:, 1:2]), axis=1)\n",
    "        train_cvr_auc = calc_auc_with_filter(cvr_input_arr, train_ctr_click)\n",
    "        if train_cvr_auc > 0:\n",
    "            train_cvr_auc_arr.append(train_cvr_auc)\n",
    "\n",
    "        ctcvr_input_arr = np.concatenate((train_ctcvr_label, train_ctcvr_prob[:, 1:2]), axis=1)\n",
    "        train_ctcvr_auc = calc_auc(ctcvr_input_arr)\n",
    "        if train_ctcvr_auc > 0:\n",
    "            train_ctcvr_auc_arr.append(train_ctcvr_auc)\n",
    "            \n",
    "        if batch_i > 0 and (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                # 累积 show_every_n_batches 个batch的Train AUC\n",
    "                print (len(train_ctr_auc_arr),len(train_cvr_auc_arr) , len(train_ctcvr_auc_arr))\n",
    "                train_ctr_auc = train_ctr_auc if len(train_ctr_auc_arr) == 0  else sum(train_ctr_auc_arr) / float(len(train_ctr_auc_arr))\n",
    "                train_cvr_auc = train_cvr_auc if len(train_cvr_auc_arr) == 0  else sum(train_cvr_auc_arr) / float(len(train_cvr_auc_arr))\n",
    "                train_ctcvr_auc = train_ctcvr_auc if len(train_ctcvr_auc_arr) == 0  else sum(train_ctcvr_auc_arr) / float(len(train_ctcvr_auc_arr))\n",
    "                # 保存 AUC\n",
    "                ctr_auc_stat['train'].append(train_ctr_auc)\n",
    "                cvr_auc_stat['train'].append(train_cvr_auc)\n",
    "                ctcvr_auc_stat['train'].append(train_ctcvr_auc)\n",
    "                # 清空，并继续累积\n",
    "                train_ctr_auc_arr.clear()\n",
    "                train_cvr_auc_arr.clear()\n",
    "                train_ctcvr_auc_arr.clear()\n",
    "                \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {} Batch {}/{}  train_loss={:.3f} train_ctr_loss={:.3f} train_cvr_loss={:.3f} train_ctcvr_loss={:.3f} train_ctr_auc={:.3f} train_cvr_auc={:.3f} train_ctcvr_auc={:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i, \n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss,\n",
    "                    train_ctr_loss,\n",
    "                    train_cvr_loss,\n",
    "                    train_ctcvr_loss,\n",
    "                    train_ctr_auc,\n",
    "                    train_cvr_auc,\n",
    "                    train_ctcvr_auc))    \n",
    "                \n",
    "        for batch_i  in range(len(test_X) // test_batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            #user_id = np.zeros([test_batch_size, 1])\n",
    "            item_id = np.zeros([test_batch_size, 1])\n",
    "            for i in range(test_batch_size):\n",
    "                #user_id[i] = x.take(0,1)[i]\n",
    "                item_id[i] = x.take(1,1)[i]\n",
    "            #User_CateIDs, User_BrandIDs\n",
    "            user_cateids = np.zeros([test_batch_size, 100])\n",
    "            for i in range(batch_size):\n",
    "                user_cateids[i] = x.take(10,1)[i]\n",
    "            user_brandids = np.zeros([test_batch_size, 100])\n",
    "            for i in range(batch_size):\n",
    "                user_brandids[i] = x.take(11,1)[i]\n",
    "            feed = {\n",
    "                UserID : np.reshape(x.take(0,1), [test_batch_size, 1]),\n",
    "                ItemID: item_id,\n",
    "                User_Cluster : np.reshape(x.take(2,1), [test_batch_size, 1]),\n",
    "                CategoryID : np.reshape(x.take(3,1), [test_batch_size, 1]),\n",
    "                ShopID : np.reshape(x.take(4,1), [test_batch_size, 1]),\n",
    "                BrandID : np.reshape(x.take(5,1), [test_batch_size, 1]),\n",
    "                Com_CateID : np.reshape(x.take(6,1), [test_batch_size, 1]),\n",
    "                Com_ShopID : np.reshape(x.take(7,1), [test_batch_size, 1]),\n",
    "                Com_BrandID : np.reshape(x.take(8,1), [test_batch_size, 1]),\n",
    "                PID : np.reshape(x.take(9,1), [test_batch_size, 1]),\n",
    "                User_CateIDs: user_cateids,\n",
    "                User_BrandIDs: user_brandids,\n",
    "                targets: np.reshape(y, [test_batch_size, 2]),\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, test_ctr_loss, test_cvr_loss, test_ctcvr_loss, \\\n",
    "                test_ctr_prob, test_cvr_prob, test_ctcvr_prob, \\\n",
    "                test_ctr_label, test_cvr_label, test_ctcvr_label, test_ctr_click,\\\n",
    "                 summaries = sess.run([global_step, loss, ctr_loss, cvr_loss, ctcvr_loss, \\\n",
    "                                    ctr_prob, cvr_prob, ctcvr_prob,\n",
    "                                    ctr_label, ctcvr_label, ctcvr_label, ctr_clk], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "            print(\"test batch click num:\", len(np.nonzero(y[:,0:1])[0]), \n",
    "                    \" buy num:\", len(np.nonzero(y[:,1:2])[0]))\n",
    "            \n",
    "            ctr_input_arr = np.concatenate((test_ctr_label, test_ctr_prob[:, 1:2]), axis=1)\n",
    "            test_ctr_auc = calc_auc(ctr_input_arr)\n",
    "            if test_ctr_auc > 0:\n",
    "                test_ctr_auc_arr.append(test_ctr_auc)\n",
    "\n",
    "            cvr_input_arr = np.concatenate((test_cvr_label, test_cvr_prob[:, 1:2]), axis=1)\n",
    "            test_cvr_auc = calc_auc_with_filter(cvr_input_arr, test_ctr_click)\n",
    "            if test_cvr_auc > 0:\n",
    "                test_cvr_auc_arr.append(test_cvr_auc)\n",
    " \n",
    "            ctcvr_input_arr = np.concatenate((test_ctcvr_label, test_ctcvr_prob[:, 1:2]), axis=1)\n",
    "            test_ctcvr_auc = calc_auc(ctcvr_input_arr)\n",
    "            if test_ctcvr_auc > 0:\n",
    "                test_ctcvr_auc_arr.append(test_ctcvr_auc)\n",
    "            \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if batch_i > 0 and (epoch_i * (len(test_X) // test_batch_size) + batch_i) % show_test_every_n_batches == 0:\n",
    "                \n",
    "                # 累积 show_every_n_batches 个batch的Train AUC\n",
    "                print (len(test_ctr_auc_arr),len(test_cvr_auc_arr) , len(test_ctcvr_auc_arr))\n",
    "                test_ctr_auc = test_ctr_auc if len(test_ctr_auc_arr) == 0  else sum(test_ctr_auc_arr) / float(len(test_ctr_auc_arr))\n",
    "                test_cvr_auc = test_cvr_auc if len(test_cvr_auc_arr) == 0  else sum(test_cvr_auc_arr) / float(len(test_cvr_auc_arr))\n",
    "                test_ctcvr_auc = test_ctcvr_auc if len(test_ctcvr_auc_arr) == 0  else sum(test_ctcvr_auc_arr) / float(len(test_ctcvr_auc_arr))\n",
    "                # 保存 AUC\n",
    "                ctr_auc_stat['test'].append(test_ctr_auc)\n",
    "                cvr_auc_stat['test'].append(test_cvr_auc)\n",
    "                ctcvr_auc_stat['test'].append(test_ctcvr_auc)\n",
    "                # 清空，并继续累积\n",
    "                test_ctr_auc_arr.clear()\n",
    "                test_cvr_auc_arr.clear()\n",
    "                test_ctcvr_auc_arr.clear()\n",
    "                \n",
    "                print('{}: Epoch {} Batch {}/{}  test_loss = {:.3f} test_ctr_loss = {:.3f} test_cvr_loss = {:.3f} test_ctcvr_loss = {:.3f}  test_ctr_auc = {:.3f} test_cvr_auc = {:.3f} test_ctcvr_auc = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // test_batch_size),\n",
    "                    test_loss,\n",
    "                    test_ctr_loss,\n",
    "                    test_cvr_loss,\n",
    "                    test_ctcvr_loss,\n",
    "                    test_ctr_auc,\n",
    "                    test_cvr_auc,\n",
    "                    test_ctcvr_auc))\n",
    "    \n",
    "    # Save Model\n",
    "    #saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
